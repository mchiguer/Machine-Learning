{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptron \n",
    "\n",
    "In this lab I will apply the Perceptron algorithm to some classification tasks. A simple implementation of the Perceptron algorithm and its application is provided. The main purpose of this session is to extend the example given to other tasks, trying to minimize test error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptron applied to the Iris dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reading the dataset:** $\\;$ I also check that the data matrix and labels have the right number of rows and columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 4) (150, 1) \n",
      " [[5.1015625  3.5        1.40039062 0.19995117 0.        ]\n",
      " [4.8984375  3.         1.40039062 0.19995117 0.        ]\n",
      " [4.69921875 3.19921875 1.29980469 0.19995117 0.        ]\n",
      " [4.6015625  3.09960938 1.5        0.19995117 0.        ]\n",
      " [5.         3.59960938 1.40039062 0.19995117 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np;\n",
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris() # Charge le dataset Iris\n",
    "X = iris.data.astype(np.float16) # Convertit les caractéristiques (features) en type float16 pour économiser de la mémoire.\n",
    "y = iris.target.astype(np.uint).reshape(-1, 1) # Convertit les cibles en entiers non signés (uint) et les redimensionne pour être une matrice colonne\n",
    "\n",
    "print(X.shape, y.shape, \"\\n\", np.hstack([X, y])[:5, :])  # Combine horizontalement les caractéristiques X et les cibles y\n",
    "# Affiche les 5 premières lignes de cette matrice "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dataset partition:** $\\;$ I create a split of the Iris dataset with $20\\%$ of data for test and the rest for training, previously shuffling the data according to a given seed provided by a random number generator. Here, as in all code that includes randomness (which requires generating random numbers), it is convenient to fix said seed to be able to reproduce experiments with accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(120, 4) (30, 4)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True, random_state=23)\n",
    "# Fonction utilisée pour diviser les données en deux sous-ensembles : un pour l'entraînement et un pour les tests\n",
    "\n",
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Perceptron implementation:** $\\;$ returns weights in homogeneous notation, $\\mathbf{W}\\in\\mathbb{R}^{(1+D)\\times C};\\;$ also the number of errors and iterations executed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np;\n",
    "def perceptron(X, y, b=0.1, a=1.0, K=200):\n",
    "    \"\"\"\n",
    "    Perceptron multi-classes.\n",
    "    \n",
    "    X : array (N, D), données d'entrée (N exemples, D caractéristiques)\n",
    "    y : array (N,), étiquettes de classe\n",
    "    b : float, biais pour le critère d'activation (par défaut 0.1)\n",
    "    a : float, taux d'apprentissage (par défaut 1.0)\n",
    "    K : int, nombre maximal d'itérations (par défaut 200)\n",
    "    \n",
    "    Retourne :\n",
    "    W : array (D+1, C), matrice des poids\n",
    "    E : int, nombre d'erreurs lors de la dernière itération\n",
    "    k : int, nombre d'itérations effectuées\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialisation des dimensions et paramètres\n",
    "    N, D = X.shape  # Nombre d'exemples (N) et de caractéristiques (D)\n",
    "    \n",
    "    Y = np.unique(y)  # Classes uniques dans y, supprime les doublons pour pouvoir savoir combien de classes on a \n",
    "    #parce que y est un vecteur qui contient a chaque position n la classe de la donnée n\n",
    "    \n",
    "    C = Y.size  # Nombre total de classes\n",
    "    W = np.zeros((1 + D, C))  # Matrice des poids, initialisée à zéro\n",
    "\n",
    "    # Boucle principale pour un maximum de K itérations et a chaque fois on itère sur tte la data \n",
    "    for k in range(1, K + 1):\n",
    "        E = 0  # Compteur d'erreurs pour cette itération\n",
    "\n",
    "        # Boucle sur chaque exemple\n",
    "        for n in range(N):\n",
    "            # Ajout d'un biais (1) à chaque ligne n au fur et a mesure que l'on parcourt les N données  et xn c est une seule ligne avec le biais 1 au debut \n",
    "            xn = np.array([1, *X[n, :]])                  \n",
    "            # Identifie la classe correcte pour cet exemple = one hot encoding \n",
    "            cn = np.squeeze(np.where(Y == y[n])) #bhala db ila eadna Y=[1,2,3] o y[n]=2 alors cn=[0,1,0]\n",
    "            # Calcul de la sortie pour la classe correcte\n",
    "            gn = W[:, cn].T @ xn #ca c est la classe prédite avec les current weights genre on prend juste la ligne des weights qui correspond à la classe correcte et on la multiplie avec la ligne de la donnée\n",
    "            # Indicateur d'erreur\n",
    "            err = False\n",
    "\n",
    "            # On trouve gn de toutes les autres classes avec les weights qu on a \n",
    "            for c in np.arange(C):\n",
    "                # On check si gn calculé avec la classe correcte est la plus grande et donc c'est celle qui va etre predite  avec les current weights \n",
    "                if c != cn and W[:, c].T @ xn + b >= gn:\n",
    "                    W[:, c] = W[:, c] - a * xn  # Diminue les poids pour la classe incorrecte\n",
    "                    err = True  # Une erreur est détectée\n",
    "            \n",
    "            # Si une erreur a été détectée, met à jour la classe correcte\n",
    "            if err:\n",
    "                W[:, cn] = W[:, cn] + a * xn  # Augmente les poids pour la classe correcte\n",
    "                E = E + 1  # Incrémente le compteur d'erreurs\n",
    "        \n",
    "        # Si aucune erreur pour tout les donnees N, cela veut dire qu ils sont tous bien classés avec les current weights, arrête l'entraînement (convergence atteinte) \n",
    "        if E == 0:\n",
    "            break\n",
    "\n",
    "    # Retourne la matrice des poids, le nombre d'erreurs et d'itérations effectuées\n",
    "    return W, E, k\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Learning a (linear) classifier with Perceptron:** $\\;$ Perceptron minimizes the number of training errors (with margin $b$)\n",
    "$$\\mathbf{W}^*=\\operatorname*{argmin}_{\\mathbf{W}=(\\boldsymbol{w}_1,\\dotsc,\\boldsymbol{w}_C)}\\sum_n\\;\\mathbb{ I}\\biggl(\\max_{c\\neq y_n}\\;\\boldsymbol{w}_c^t\\boldsymbol{x}_n+b \\;>\\; \\boldsymbol{w}_{y_n}^t\\boldsymbol{ x}_n\\biggr)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of iterations executed:  200\n",
      "Number of training errors:  2\n",
      "Weight vectors of the classes (in columns and with homogeneous notation):\n",
      " [[  10.           85.         -142.        ]\n",
      " [ -49.421875    -68.19140625 -176.47265625]\n",
      " [  50.171875     -1.72460938 -181.06445312]\n",
      " [-189.91210938  -87.70507812   68.69726562]\n",
      " [ -86.40258789 -137.78149414  157.88415527]]\n"
     ]
    }
   ],
   "source": [
    "#Dans ce cas la, ca s est pas naturellement arrete donc ca a juste atteint le nombre max d iterations et du cp ca a donne les weights de la dernière iteration \n",
    "W, E, k = perceptron(X_train, y_train) \n",
    "print(\"Number of iterations executed: \", k)\n",
    "print(\"Number of training errors: \", E)\n",
    "print(\"Weight vectors of the classes (in columns and with homogeneous notation):\\n\", W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"page-break-after:always;\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calculation of test error rate:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error rate on test: 16.7%\n"
     ]
    }
   ],
   "source": [
    "X_testh = np.hstack([np.ones((len(X_test), 1)), X_test]) # Combine une colonne de 1 (biais) à X_test\n",
    "y_test_pred  = np.argmax(X_testh @ W, axis=1).reshape(-1, 1) # Calcule les prédictions pour l'ensemble de test = trouve l'indice (classe) avec le score maximal pour chaque exemple.\n",
    "err_test = np.count_nonzero(y_test_pred != y_test) / len(X_test) # Calcule le taux d'erreur sur l'ensemble de test\n",
    "print(f\"Error rate on test: {err_test:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Margin adjustment:** $\\;$ experiment to learn a value of $b$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 3 1000\n",
      "0.01 5 1000\n",
      "0.1 3 1000\n",
      "10 6 1000\n",
      "100 6 1000\n"
     ]
    }
   ],
   "source": [
    "# Teste le perceptron avec différents biais (b) et évalue ses performances\n",
    "for b in (.0, .01, .1, 10, 100): # Liste de valeurs pour le biais\n",
    "    W, E, k = perceptron(X_train, y_train, b=b, K=1000) # Entraîne le perceptron avec le biais b, un maximum de 1000 itérations\n",
    "    print(b, E, k) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation of results:** $\\;$ the training data does not appear to be linearly separable; it is not clear that a margin greater than zero can improve results, especially since we only have $30$ test samples; with a margin $b=0.1$ we have already seen that an error (in test) of $16.7\\%$ is obtained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (base)",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
